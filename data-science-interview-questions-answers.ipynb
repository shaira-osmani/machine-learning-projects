{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0448,
     "end_time": "2022-11-08T11:13:32.899070",
     "exception": false,
     "start_time": "2022-11-08T11:13:32.854270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Important Tips\n",
    "* Datascience interview questions can include questions from statistics, math, data visualization, analytics, software engineering, baics ML concepts, ML models etc. Type of questions can also vary from being fiexed answer questions or open ended questions where multiple solutions are possible.\n",
    "* Please **DO NOT TRY TO REMEMBER** these question answers. Instead use this notebook to test your knowledge and accordingly work on the weak areas.\n",
    "* Try to understand what interviewer is trying to know from each question and explain it in your own words.\n",
    "* Best way of learning anything new is to read about it -> understand the concepts -> try it your self -> if possible publish your learning and colloborate with other learners.\n",
    "* I will keep updating this notebook as and when I come across interesting questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040955,
     "end_time": "2022-11-08T11:13:32.982410",
     "exception": false,
     "start_time": "2022-11-08T11:13:32.941455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Suppose you had bank transaction data, and wanted to separate out likely fraudulent transactions. How would you approach it? Why might accuracy be a bad metric for evaluating success? \n",
    "* In Machine Learning, problems like fraud detection are usually framed as classification problems. In order to solve this problem we may use different features like amount, merchant, location, time etc associated with each transaction.\n",
    "* One of the biggest challenge with fraud transaction detection is- majority of transactions are not fraud, so we have inbalance data!\n",
    "* First step will be to do EDA and understand our data and intesity of class inbalance.\n",
    "* In order to handle inbalance data problem we can use one of the following method\n",
    "    * Oversampling — SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "    * Undersampling — One simple way of undersampling is randomly selecting a handful of samples from the class that is overrepresented.\n",
    "    * Combined Class Methods — Use SMOTE together with edited nearest-neighbours (ENN). Here, ENN is used as the cleaning method after SMOTE over-sampling to obtain a cleaner space.\n",
    "        * Developed by Wilson (1972), the ENN method works by finding the K-nearest neighbor of each observation first, then check whether the majority class from the observation’s k-nearest neighbor is the same as the observation’s class or not. \n",
    "        * If the majority class of the observation’s K-nearest neighbor and the observation’s class is different, then the observation and its K-nearest neighbor are deleted from the dataset. In default, the number of nearest-neighbor used in ENN is K=3.\n",
    "        * As ENN removes the observation and its K-nearest neighbor instead of just removing observation and its 1-nearest neighbor that are having different classes. Thus, ENN can be expected to give more in-depth data cleaning.\n",
    "* Test model performane for each of above technique and choose best performing model.\n",
    "\n",
    "### Why might accuracy be a bad metric for evaluating success? \n",
    "* In case of inbalance data accuracy metric is not usefull. Accuracy tells us how close a measured value is to the actual (true) value. But here we are more interested in fraud transactions.\n",
    "* We dont mind declaring few good transactions as fruad but failing to identify fraud transaction is not acceptable. In such cases classification of true positives is a priority, hence precision metric make more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042307,
     "end_time": "2022-11-08T11:13:33.064849",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.022542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain inner working on linear regression\n",
    "https://satishgunjal.com/univariate_lr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03867,
     "end_time": "2022-11-08T11:13:33.143428",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.104758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What are the assumptions for linear regression\n",
    "Linear regression assumptions are as below\n",
    "* Data should have linear relationship between X and Y (actually mean of Y)\n",
    "* Data should be normally distributed\n",
    "* No or little multicollinearity (observations should be independent of each other)\n",
    "* Homoscedasticity- There should not be unequal variance in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038902,
     "end_time": "2022-11-08T11:13:33.222210",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.183308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain inner working on logistic regression\n",
    "https://satishgunjal.com/binary_lr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03885,
     "end_time": "2022-11-08T11:13:33.300522",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.261672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How can AI be used in spam email detection?\n",
    "* First we collect spam and ham email data\n",
    "* Then we find the statistical relations between words to create feature matrix to train the classification models.\n",
    "* Trained classification models can be used to determine whether a piece of text belongs to a certain class.\n",
    "* We can use algorithms like Naïve Bayes, RNN and transformers for spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03916,
     "end_time": "2022-11-08T11:13:33.379674",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.340514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What are the advantages and disadvantages of neural networks?  \n",
    "\n",
    "**Here are some advantages of Neural Networks**\n",
    "\n",
    "* Storing information on the entire network: Information such as in traditional programming is stored on the entire network, not on a database. The disappearance of a few pieces of information in one place does not restrict the network from functioning. \n",
    "* The ability to work with inadequate knowledge: After ANN training, the data may produce output even with incomplete information. The lack of performance here depends on the importance of the missing information. \n",
    "* It has fault tolerance:  Corruption of one or more cells of ANN does not prevent it from generating output. This feature makes the networks fault-tolerant. \n",
    "* Having a distributed memory: For ANN to be able to learn, it is necessary to determine the examples and to teach the network according to the desired output by showing these examples to the network. The network's progress is directly proportional to the selected instances, and if the event can not be shown to the network in all its aspects, the network can produce incorrect output \n",
    "* Gradual corruption:  A network slows over time and undergoes relative degradation. The network problem does not immediately corrode.\n",
    "* Ability to train machine: Artificial neural networks learn events and make decisions by commenting on similar events. \n",
    "* Parallel processing ability:  Artificial neural networks have numerical strength that can perform more than one job at the same time. \n",
    "\n",
    "**Disadvantages of Neural Networks**\n",
    "\n",
    "* Hardware dependence:  Artificial neural networks require processors with parallel processing power, by their structure. For this reason, the realization of the equipment is dependent. \n",
    "* Unexplained functioning of the network: This is the most important problem of ANN. When ANN gives a probing solution, it does not give a clue as to why and how. This reduces trust in the network. \n",
    "* Assurance of proper network structure:  There is no specific rule for determining the structure of artificial neural networks. The appropriate network structure is achieved through experience and trial and error. \n",
    "* The difficulty of showing the problem to the network:  ANNs can work with numerical information. Problems have to be translated into numerical values before being introduced to ANN. The display mechanism to be determined here will directly influence the performance of the network. This depends on the user's ability. \n",
    "* The duration of the network is unknown: The network is reduced to a certain value of the error on the sample means that the training has been completed. This value does not give us optimum results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040569,
     "end_time": "2022-11-08T11:13:33.459555",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.418986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is the difference between bias and variance?\n",
    "* Bias comes from model underfitting some set of data, whereas variance is the result of model overfitting some set of data.\n",
    "* Underfitting models have high error in training as well as test set. This behavior is called as ‘Low Bias’\n",
    "* Consider below example of bias(underfitting) where we are trying to fit linear function for nonlinear data.\n",
    "\n",
    "![Underfitting](https://raw.githubusercontent.com/satishgunjal/images/master/Underfitting.png)\n",
    "* Overfitting models have low error in training set but high error in test set. This behavior is called as ‘High Variance’\n",
    "* Consider below example of variance(overfitting) where complicated function creates lots of unnecessary curves and angles that are not related with data.\n",
    "\n",
    "![Overfitting](https://raw.githubusercontent.com/satishgunjal/images/master/Overfitting.png)\n",
    "\n",
    "* Low bias (low underfitting) ML algorithms: Decision Tree, k-NN, SVM\n",
    "* High bias (high underfitting) ML algorithms: Linear regression, Logistic regression\n",
    "* High Variance (high overfitting) ML algorithms: Polynimial regression\n",
    "* Reference: https://satishgunjal.com/underfitting_overfitting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039774,
     "end_time": "2022-11-08T11:13:33.540315",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.500541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is bias-variance tradeoff\n",
    "* As we increase the complexity of the model, error will reduce due to lower bias in the model. However, this will happen until a particular point. If we continue to make our model complex then model will overfit and lead to high variance.\n",
    "* The goal of any supervised ML algorithm to have low bias and low variance to achieve good prediction performance. This is referred as bias-variance tradeoff. We can acheive bias-variance tradeoff by selecting optimum model complexity.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/satishgunjal/images/master/Bias_and_variance_contributing_to_total_error.png\" width=\"500\" height=\"400\">\n",
    "\n",
    "**We can also use hyperparamters to adjust model complexity, few examples are as below**\n",
    "\n",
    "* The K-NN algorithm has low bias(underfitting) and high variance(overfitting), tradeoff can be achieved by increasing the value of 'K'. \n",
    "    * Higher the value of 'K' means higher the number of neighbours, which in turn increases the bias of the model.\n",
    "* The SVM algorithm has low bias(underfitting) and high variance(overfitting), trade off can be achived by changing the 'C' paramter.\n",
    "    * The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. \n",
    "    * For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. \n",
    "    * Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. \n",
    "    * For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.\n",
    "* The decision trees has low bias(underfitting) and high variance(overfitting), bias-variance tradeoff can be achived by changing the tree depth.\n",
    "    * If the tree is shallow then we're not checking a lot of conditions/constrains i.e. the logic is simple or less complex, hence it automatically reduces over-fitting. This introduces more bias compared to deeper trees where we overfit the data. It can be imagined as we're deliberately not calculating more conditions means we're making some assumption (introduces bias) while creating the tree.\n",
    "* The linear regression has low variance(overfitting) and high bias(underfitting), bias-variance tradeoff can be acheived by increasing the number of features or by using another regression technique that can fit data better.\n",
    "    * If data is not linearly separable then linear regression algorithm will result in low variance and highj bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039868,
     "end_time": "2022-11-08T11:13:33.621992",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.582124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is more important model accuracy or model performance?\n",
    "* Short answer is: Model accuracy matters the most! inaccurate information is not usefull.\n",
    "* Model performance can be improved by increasing the compute resources.\n",
    "* Model accuracy and performance can be subjective to the problem in hand. For example, in analysis of medical images to determine if there is a disease (such as cancer), the accuracy extremely critical, even if the models would take minutes or hours to make a prediction.\n",
    "* Some applications require real time performance, even if this comes at a cost of accuracy. For example, imagine a machine that views a fast conveyor belt carrying tomatoes, where it must separate the green from the red ones. Though an occasional error is undesired, the success of this machine is more determined by its ability to withstand its throughput.\n",
    "* A more common example is face detection for recreational applications. People would expect a fast response from the app, though the occasional missed face would not render it useless.\n",
    "* Reference: https://www.quora.com/Which-is-more-important-to-you-model-accuracy-or-model-performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043928,
     "end_time": "2022-11-08T11:13:33.706747",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.662819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is the difference between machine learning and deep learning?\n",
    "Deep Learning out performs traditional ML techniques if the data size is large. But with small data size, traditional Machine Learning algorithms are preferable. Deep Learning really shines when it comes to complex problems such as image classification, natural language processing, and speech recognition. Few important differences are as below,\n",
    "\n",
    "|Machine Learning|Deep Learning|\n",
    "|:-|:-|\n",
    "| Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned|Deep learning structures algorithms in layers to create an \"artificial neural network” that can learn and make intelligent decisions on its own|\n",
    "|Using handcrafted rules and feature engineering, ML algorithms can work well with small data. But its performance plateau once data increases.|Deep Learning algorithms need large data to understand it perfectly. Deep learning performances increases as data increases.|\n",
    "|Traditional ML algorithms can work on less computing power.|DL algorithms need high compute. There also special purpose compute for DL like GPu and TPU.|\n",
    "|In case of ML domain experts/Data scientists needs to do feature engineering in order to enable model o learn all data patterns.|One advantage with DL that it learns high level features from data.No extrnal feature engineering is required.|\n",
    "|ML models take less time to train|DL models take more time to train|\n",
    "| Ml models are easy to interpret as comare to DL models.|DL models are black box and its very difficult to interpret the results.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039719,
     "end_time": "2022-11-08T11:13:33.786607",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.746888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain standard deviation and variance\n",
    "![normal-distrubution](https://www.mathsisfun.com/data/images/normal-distrubution-1sd.svg)\n",
    "\n",
    "* Standard deviation is measure of how spread out numbers are\n",
    "    Formula σ = square root of the Variance\n",
    "* Variance is defined as “The average of the squared differences from the Mean.”\n",
    "* Using the Standard Deviation we have a \"standard\" way of knowing what is normal, and what is extra large or extra small.\n",
    "* We can expect about 68% of values to be within plus-or-minus 1 standard deviation.\n",
    "* A low standard deviation indicates that the data points tend to be very close to the mean; a high standard deviation indicates that the data points are spread out over a large range of values\n",
    "* Ref. https://www.mathsisfun.com/data/standard-deviation.html#Top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042697,
     "end_time": "2022-11-08T11:13:33.870671",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.827974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain confusion matrix\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/Basic-Confusion-matrix.png)\n",
    "\n",
    "* The confusion matrix is one of the most powerful tools for predictive analysis in machine learning. \n",
    "* A confusion matrix gives you information about how your machine classifier has performed, pitting properly classified examples against misclassified examples.\n",
    "* Confusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision. \n",
    "* Confusion matrices are useful because they give direct comparisons of values like True Positives, False Positives, True Negatives and False Negatives. In contrast, other machine learning classification metrics like “Accuracy” give less useful information, as Accuracy is simply the difference between correct predictions divided by the total number of predictions.\n",
    "* All estimation parameters of the confusion matrix are based on 4 basic inputs namely True Positive, False Positive, True Negative and False Negative.\n",
    "* Confusion matrices have two types of errors: Type I (False Positive) and Type II (False Negative). False Positive contains one negative word (False) so it’s a Type I error. False Negative has two negative words (False + Negative) so it’s a Type II error.\n",
    "* From our confusion matrix, we can calculate five different metrics measuring the validity of our model.\n",
    "    * **ACCURACY**\n",
    "    \n",
    "        Accuracy is the ratio of correctly identified subjects in a pool of subjects.\n",
    "\n",
    "        Accuracy = (all correct / all) = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "        Accuracy answers the question: How many patients did we correctly identify out of all patients?\n",
    "\n",
    "    * **PRECISION**\n",
    "    \n",
    "        Precision is the ratio of correctly identified +ve subjects by test, against all +ve subjects identified by test.\n",
    "\n",
    "        Precision =  (true positives / predicted positives) = TP/(TP+FP)\n",
    "\n",
    "        Precision answers the question: How many patients tested +ve are actually +ve?\n",
    "\n",
    "        This metric is often used in cases where classification of true positives is a priority. For example, a spam email classifier would rather classify some spam emails as regular emails rather than classify some regular emails as spam. That’s why some spam emails end up in your main inbox, just to be safe. (Here true positives are the spam emails)\n",
    "        \n",
    "    * **SENSITIVITY (RECALL)**\n",
    "    \n",
    "        Sensitivity is the ratio of correctly identified +ve subjects by test against all +ve subjects in reality.\n",
    "\n",
    "        Sensitivity =  (true positives / all actual positives)= TP/(TP+FN)\n",
    "\n",
    "        Sensitivity answers the question: Of all the patients that are +ve, how many did the test correctly predict?\n",
    "\n",
    "        This metric is often used in cases where classification of false negatives is a priority. A good example is the medical test that we used for illustration above. The government would rather have some healthy people labeled +ve than have an infected individual labeled -ve and spread the disease. We would rather be overly cautious and have false positives than risk wrongly identifying false negatives.\n",
    "\n",
    "    * **SPECIFICITY**\n",
    "    \n",
    "        Specificity is the ratio of correctly identified -ve subjects by test against all -ve subjects in reality.\n",
    "\n",
    "        Specificity =  (true negatives / all actual negatives) = TN/(TN+FP)\n",
    "\n",
    "        Specificity answers the question: Of all the patients that are -ve, how many did the test correctly predict?\n",
    "\n",
    "        This metric is often used in cases where classification of true negatives is a priority. For example, a doping test will immediately ban an athlete if they are tested positive. We would not want to any drug-free athlete to be wrongly classified and banned.\n",
    "\n",
    "    * **F1 SCORE**\n",
    "    \n",
    "        F1 Score accounts for both precision and sensitivity.\n",
    "\n",
    "        F1 Score = 2 * (Recall * Precision)/(Recall + Precision)\n",
    "\n",
    "        It is often considered a better indicator of a classifier’s performance than a regular accuracy measure as it compensates for uneven class distribution in the training dataset. For example, an uneven class distribution is likely to occur in insurance fraud detection, where a large majority of claims are legitimate and only a very small minority are fraudulent. \n",
    "\n",
    "**Which metric to use is depends on the problem in hand**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040995,
     "end_time": "2022-11-08T11:13:33.953935",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.912940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Why do we need confusion matrix?\n",
    "* We can not rely on a single value of accuracy in classification when the classes are imbalanced. \n",
    "* For example, we have a dataset of 100 patients in which 5 have diabetes and 95 are healthy. However, if our model only predicts the majority class i.e. all 100 people are healthy then also we will have a classification accuracy of 95%.\n",
    "* Confusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision. \n",
    "* Confusion matrices are useful because they give direct comparisons of values like True Positives, False Positives, True Negatives and False Negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041328,
     "end_time": "2022-11-08T11:13:34.036178",
     "exception": false,
     "start_time": "2022-11-08T11:13:33.994850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain collinearity and technique to reduce it?\n",
    "In statistics collinearity or multicollinearity is the phenomenon where one or more predictive variables(features) in multiple regression models are highly linearly related to each other.\n",
    "## Technique to reduce multicollearity\n",
    "* **Remove highly correlated predictors from the model**.  If you have two or more factors with a high collinearity, remove one from the model. Because they supply redundant information, removing one of the correlated factors usually doesn't drastically reduce the R-squared.  Consider using stepwise regression, best subsets regression, or specialized knowledge of the data set to remove these variables. Select the model that has the highest R-squared value. \n",
    "* **Principal Components Analysis(PCA)** regression methods that cut the number of predictors to a smaller set of uncorrelated components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040269,
     "end_time": "2022-11-08T11:13:34.117427",
     "exception": false,
     "start_time": "2022-11-08T11:13:34.077158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Difference between statistics and machine learning\n",
    "* The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables.\n",
    "* Statistics is mathematical study of data. Lots of statistical models that can make predictions, but predictive accuracy is not their strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039378,
     "end_time": "2022-11-08T11:13:34.196108",
     "exception": false,
     "start_time": "2022-11-08T11:13:34.156730",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# In a test, students in section A scored with a mean of 75 and standard deviation of 10, while students in section B scored with a mean of 80 and standard deviation of 12? Melissa from section A and Ryan from section B both have scored 90 in this test. Who had a better performance in this test as compared to their classmates?\n",
    "\n",
    "To compare the two scores we need to standardize them to the same scale. We do that by calculating the Z score, which allows us to compare the 2 scores in units of standard deviations. \n",
    "\n",
    "```Z score= (X- mean)/Standard Deviation```\n",
    "\n",
    "Melissa's Z score = (90-75)/10 = 1.5\n",
    "\n",
    "Ryan's Z score = (90-80)/12 = 0.83\n",
    "\n",
    "Melissa has performed better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040541,
     "end_time": "2022-11-08T11:13:34.279735",
     "exception": false,
     "start_time": "2022-11-08T11:13:34.239194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is null hypothesis and alternate hypothesis?\n",
    "* The null hypothesis states that a population parameter (such as the mean, the standard deviation, and so on) is equal to a hypothesized value. The null hypothesis is often an initial claim that is based on previous analyses or specialized knowledge.\n",
    "* The alternative hypothesis states that a population parameter is smaller, greater, or different than the hypothesized value in the null hypothesis. The alternative hypothesis is what you might believe to be true or hope to prove true.\n",
    "* So when running a hypothesis test/experiment, the null hypothesis says that there is no difference or no change between the two tests. The alternate hypothesis is the opposite of the null hypothesis and states that there is a difference between the two tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040979,
     "end_time": "2022-11-08T11:13:34.361238",
     "exception": false,
     "start_time": "2022-11-08T11:13:34.320259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is a hypothesis test and p-value?\n",
    "* A hypothesis test examines two opposing hypotheses about a population: the null hypothesis and the alternative hypothesis. The null hypothesis is the statement being tested. Usually the null hypothesis is a statement of \"no effect\" or \"no difference\". The alternative hypothesis is the statement you want to be able to conclude is true based on evidence provided by the sample data.\n",
    "* Based on the sample data, the test determines whether to reject the null hypothesis. You use a p-value, to make the determination. If the p-value is less than the significance level (denoted as alpha), then you can reject the null hypothesis.\n",
    "* In laymans term the p-value is the probability that the null hypothesis is true.\n",
    "* Consider the example where we are trying to test whether a new marketing campaign generates more revenue\n",
    "    * Here null hypothesis states that there is no change in the revenue as a result of the new marketing campaign\n",
    "    * Based on p-value we can accept or reject the null hypothesis. 0.25 p-value means there is 25% chance that new marketing campaign will not change revenue\n",
    "    * Lower the p-value, the more confident we are that the alternate hypothesis is true, which, in this case, means that the new marketing campaign causes an increase or decrease in revenue.\n",
    "    * In most fields, acceptable p-values should be under 0.05 while in other fields a p-value of under 0.01 is required.\n",
    "    * So when a result has a p-value of 0.05 or lower we can reject null hypothesis and accept the alternate hypothesis. \n",
    "    \n",
    "## More Info: Basic Concepts of Hypothesis Testing\n",
    "\n",
    "* In simple term hypothesis is a assumption. Since its assumption, after our testing it can hold true may not.\n",
    "* If our assumption holds true after testing then it is termed as 'Null Hypothesis' unless there is evidence against it.\n",
    "* If our assumption dont hold true and there is claim aganist it, then it is termed as alternate hypothesis.\n",
    "* So when our assumption dont hold true then Type I error occurs. Here we are going against the Null Hypothesis.\n",
    "* p-value: It is calculated probability of making type I error\n",
    "* Reference: https://www.youtube.com/watch?v=d0eVIUyt_Uc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038842,
     "end_time": "2022-11-08T11:13:34.442729",
     "exception": false,
     "start_time": "2022-11-08T11:13:34.403887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is power of hypothesis test? Why is it important?\n",
    "\n",
    "* Remember that if actual value is positive and our model predicts it as negative then Type II error occuras (False negative). e.g. Calling a guilty person innocent, diaognosing cancer infected person as healthy etc. \n",
    "* The probability of not commiting Type II error is called as power of hypothesis test. The higher probability we have of not commiting a type 2 error, the better our hypothesis test is.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03907,
     "end_time": "2022-11-08T11:13:34.521522",
     "exception": false,
     "start_time": "2022-11-08T11:13:34.482452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is the difference betweeen K nearest neighbors and K means\n",
    "* KNN or K nearest neighbor is a classification algorithm, while K-Means is clustering technique.\n",
    "* KNN is supervised algorithm, K means is unsupervised algorithm.\n",
    "* In KNN prediction of the test sample is based on the similarity of its features to its neighbors. The similarity is computed based on the measure such as euclidean distance. Here K referes to the number of neighbors with whom similarity is being compared.\n",
    "* K-means is the process of defining clusters or groups around predefined centroids based on the similarity of each data point to each other. Here K referes to the number of centroids around which clusters will be formed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.055356,
     "end_time": "2022-11-08T11:13:34.620725",
     "exception": false,
     "start_time": "2022-11-08T11:13:34.565369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain Random forest algorithm\n",
    "* Random forest is supervised learning algorithm and can be used to solve classification and regression problems. \n",
    "* Since decision-tree create only one tree to fit the dataset, it may cause overfitting and model may not generalize well. Unlike decision tree random forest fits multiple decision trees on various sub samples of dataset and make the predictions by averaging the predictions from each tree. \n",
    "* Averaging the results from multiple decision trees help to control the overfitting and results in much better prediction accuracy. As you may have noticed, since this algorithm uses multiple trees hence the name ‘Random Forest’\n",
    "* Reference: [Random Forest](https://satishgunjal.com/random_forest/)\n",
    "* This algorithm is heavily used in various industries such as Banking and e-commerce to predict behavior and outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038802,
     "end_time": "2022-11-08T11:13:34.710148",
     "exception": false,
     "start_time": "2022-11-08T11:13:34.671346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Can Random Forest Algorithm be used both for Continuous and Categorical Target Variables?\n",
    "* Yes, Random Forest can be used for both continuous and categorical target (dependent) variables.\n",
    "* In a random forest the classification model refers to the categorical dependent variable, and the regression model refers to the numeric or continuous dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.048495,
     "end_time": "2022-11-08T11:13:34.798578",
     "exception": false,
     "start_time": "2022-11-08T11:13:34.750083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What do you mean by Bagging?\n",
    "\n",
    "![EnsembleI_Learning_Bagging](https://raw.githubusercontent.com/satishgunjal/images/master/Ensemble_Learning_Bagging.png)\n",
    "\n",
    "* In bagging we build independent estimators on different samples of the original data set and average or vote across all the predictions.\n",
    "* Bagging is a short form of **Bootstrap Aggregating**. It is an ensemble learning approach used to improve the stability and accuracy of machine learning algorithms.\n",
    "* Since multiple model predictions are averaged together to form the final predictions, Bagging reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method.\n",
    "* Bagging is a special case of the model averaging approach, in case of regression problem we take mean of the output and in case of classification we take the majority vote.\n",
    "* Bagging is more helpfull if we have over fitting (high variance) base models.\n",
    "* We can also build independent estimators of same type on each subset. These independent estimators also enable us to parallelly process and increase the speed.\n",
    "* Most popular bagging estimator is 'Bagging Tress' also knows as 'Random Forest'\n",
    "\n",
    "## Bootstrapping\n",
    "\n",
    "* It is a resampling technique, where large numbers of smaller samples of the same size are repeatedly drawn, with replacement, from a single original sample.\n",
    "* So this technique will enable us to produce as many subsample as we required from the original training data.\n",
    "* The defination is simple to understand, but \"replacement\" word may be confusing sometimes. Here 'replacement' word signifies that the same obervation may repeat more than once in a given sample, and hence this technique is also known as **sampleing with replacement**\n",
    "\n",
    "![Bootstrap_Sampling_ML](https://raw.githubusercontent.com/satishgunjal/images/master/Bootstrap_Sampling_ML.png)\n",
    "\n",
    "* As you can see in above image we have training data with observations from X1 to X10. In first bootstrap training sample X6, X10 and X2 are repeated where as in second training sample X3, X4, X7 and X9 are repeated.\n",
    "* Bootstrap sampling helps us to generate random sample from given training data for each model in order to genralise the final estimation.\n",
    "* So in case of Bagging we create multiple number of bootstrap samples from given data to train our base models. Each sample will contain training and test data sets which are different from each other and remember that training sample may contain duplicate observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03903,
     "end_time": "2022-11-08T11:13:34.891898",
     "exception": false,
     "start_time": "2022-11-08T11:13:34.852868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is Out-of-Bag Error in Random Forests?\n",
    "* Out-of-Bag is equivalent to validation or test data but it is calculated internally by Random Forest algorithm. In case of Sklearn if we set hyperparameter 'oob_score =  True' then Out-of-Bag score will be calculated for every decision tree.\n",
    "* Finally, we aggregate all the errors from all the decision trees and we will determine the overall OOB error rate for the classification.\n",
    "* For more details refer. https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.044983,
     "end_time": "2022-11-08T11:13:34.975936",
     "exception": false,
     "start_time": "2022-11-08T11:13:34.930953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is the use of proximity matrix in the random forest algorithm?\n",
    "A proximity matrix is used for the following cases :\n",
    "* Missing value imputation\n",
    "* Detection of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.045015,
     "end_time": "2022-11-08T11:13:35.066103",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.021088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# List down the parameters used to fine-tune the Random Forest.\n",
    "Two parameters that have to fine-tune to improve the predictions that are important in the random forest algorithm are as follows:\n",
    "* Number of trees used in the forest (n_tree)\n",
    "* Number of random variables used in each of the trees in the forest (mtry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038897,
     "end_time": "2022-11-08T11:13:35.144848",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.105951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is K Fold cross validation? Why do you use it?\n",
    "* In case of K Fold cross validation input data is divided into ‘K’ number of folds, hence the name K Fold. Suppose we have divided data into 5 folds i.e. K=5. Now we have 5 sets of data to train and test our model. So the model will get trained and tested 5 times, but for every iteration we will use one fold as test data and rest all as training data. Note that for every iteration, data in training and test fold changes which adds to the effectiveness of this method.\n",
    "* This significantly reduces underfitting as we are using most of the data for training(fitting), and also significantly reduces overfitting as most of the data is also being used in validation set.\n",
    "* K Fold cross validation helps to generalize the machine learning model, which results in better predictions on unknown data. \n",
    "* Reference: [K Fold Cross Validation](https://satishgunjal.com/kfold/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039109,
     "end_time": "2022-11-08T11:13:35.222923",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.183814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How to handle missing data?\n",
    "Data can be missing because of mannual error or can be gennualy missing.\n",
    "\n",
    "* Delete low quality records completely which have too much missing data\n",
    "* Impute the values by educated guess, taking average or regression\n",
    "* Use domain knwledge to impute values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040276,
     "end_time": "2022-11-08T11:13:35.302832",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.262556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is the difference between Bar graph and histogram?\n",
    "\n",
    "* Bar graph is used for descreate data where as histogram is used for continuous data.\n",
    "* In bar graph there is space between the bars and in case of histogram there is no space between the bars(contnuous scale).\n",
    "* In bar graph the order of the bars can be changed and in histogram order remains same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0391,
     "end_time": "2022-11-08T11:13:35.381989",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.342889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is the Box and Whisker plot? When should use it?\n",
    "* Box and whisker plots are ideal for comparing distributions because the centre, spread and overall range are immediately apparent. \n",
    "* A box and whisker plot is a way of summarizing a set of data measured on an interval scale. \n",
    "* It is often used in explanatory data analysis\n",
    "* Boxplots are a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”).\n",
    "    * median (Q2/50th Percentile): the middle value of the dataset.\n",
    "    * first quartile (Q1/25th Percentile): the middle number between the smallest number (not the “minimum”) and the median of the dataset.\n",
    "    * third quartile (Q3/75th Percentile): the middle value between the median and the highest value (not the “maximum”) of the dataset.\n",
    "    * interquartile range (IQR): 25th to the 75th percentile.\n",
    "    * whiskers (shown in blue)\n",
    "    * outliers (shown as green circles)\n",
    "    * “maximum”: Q3 + 1.5*IQR\n",
    "    * “minimum”: Q1 -1.5*IQR\n",
    "    \n",
    "    ![](https://miro.medium.com/max/2400/1*2c21SkzJMf3frPXPAR_gZA.png)\n",
    "    \n",
    "    Ref. https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039104,
     "end_time": "2022-11-08T11:13:35.460050",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.420946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is outlier? How to handle them?\n",
    "\n",
    "* An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.\n",
    "* Data points above and below 1.5*IQR, are most commonly outliers.\n",
    "\n",
    "Outliers can drastically change the results of the data analysis and statistical modeling.\n",
    "\n",
    "    \n",
    "## Types of the outliers\n",
    "* **Data entry errors**\n",
    "* **Measuremental errors**\n",
    "* **Intentional outliers**. This is commonly found in self-reported measures that involves sensitive data. For example: Teens would typically under report the amount of alcohol that they consume.\n",
    "* **Data processing erros**. Whenever we perform data mining, we extract data from multiple sources. It is possible that some manipulation or extraction errors may lead to outliers in the dataset.\n",
    "* **Sampling error**. For instance, we have to measure the height of athletes. By mistake, we include a few basketball players in the sample. This inclusion is likely to cause outliers in the dataset.\n",
    "* **Natutal oulier**. When an outlier is not artificial (due to error), it is a natural outlier. For instance: In my problem assignment with one of the renowned insurance company, I noticed that the performance of top 50 financial advisors was far higher than rest of the population. Surprisingly, it was not due to any error. Hence, whenever we perform any data mining activity with advisors, we used to treat this segment separately.\n",
    "    \n",
    "\n",
    "## How to detect Outliers?\n",
    "Most commonly used method to detect outliers is visualization.\n",
    "* We use various visualization methods, like Box-plot, Histogram, Scatter Plot \n",
    "* Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier\n",
    "* Data points, three or more standard deviation away from mean are considered outlier\n",
    "\n",
    "Apart from visualization we can also use Z-Score or Extreme Value Analysis (parametric) to detect outliers.\n",
    "\n",
    "## How to remove outliers?\n",
    "Most of the methods used to handle missing values are aslo application in case ot outliers\n",
    "\n",
    "### Deleting observations\n",
    "We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.\n",
    "\n",
    "### Transforming and binning values\n",
    "Transforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allows to deal with outliers well due to binning of variable.\n",
    "\n",
    "### Imputing\n",
    "We can use mean, median, mode imputation methods.\n",
    "\n",
    "### Treat separately\n",
    "If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output.\n",
    "\n",
    "Reference: https://www.linkedin.com/pulse/techniques-outlier-detection-treatment-suhas-jk/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040318,
     "end_time": "2022-11-08T11:13:35.540314",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.499996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# If deleting outliers is not an option, how will you handle them?\n",
    "* I will try differen models. Data detected as outliers by linear model, can be fit by  non-linear model.\n",
    "* Try normalizing the data, this way the extreame datapoints are pulled to the similar range.\n",
    "* We can use algorithms which are less affected by outliers.\n",
    "* We can also create separate model to handle the outlier data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042058,
     "end_time": "2022-11-08T11:13:35.623214",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.581156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# You fit two linear models on a dataset. Model 1 has 25 predictors and model 2 has 10 predictors. What performance metric would you use to select the best model based on training dataset?\n",
    "\n",
    "* First of all model performace is not directly proportional to the number of predictors, so we cant say that model with 25 predictors is better than the model with 10 predictors\n",
    "* Here important thing is to understand different evaluation metric for linear regresion and which one of them can help us identify the impact of number of predictors on model performance.\n",
    "* Evaluation metric used for linear regression are MSE, MAE, R-squared, Adjusted R-squared, and RMSE.\n",
    "* MSE penalizes large errors, MAE does not penalize large errors, RMSE penalizes large errors and R-squared or Coefficient of Determination represent the strength of the relationship between your model and the dependent variable.\n",
    "* Though R-squared represent the strength of relationship between model and the dependent variables, it is never used for comparing the models as the value of R² increases with the increase in the number of predictors (even if these predictors do not add any value to the model)\n",
    "* Now only remaining metric is **Adjusted R-squared**. Unlike R-squared, Adjusted R-squared measures variation explained by only the independent variables that actually affect the dependent variable.\n",
    "* So the Adjusted R-squre score will increase only if addition of predictors improve the models performance significantly or else it will decrease. Hence correct answer is Adjusted R-squared\n",
    "\n",
    "* Reference: https://www.youtube.com/watch?v=lRAgottY8XU&list=PLjW9PIyfCennBOprV3CPoqMX8SW-qNlUa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040702,
     "end_time": "2022-11-08T11:13:35.703770",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.663068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Suppose we have a function -4x^2 + 4x + 3. Find the maximum or minimum of this function.\n",
    "* This is quadratic equation,\n",
    "  f(x) = -4x^2 + 4x + 13 (for a function: ax^2 + bx + c, when a < 0, then function has maximum value)\n",
    "* To find the slope of the function, lets take derivative of it\n",
    "\n",
    "  f'(x)= -8x + 4\n",
    "  \n",
    "* At maximum point, slope will be 0\n",
    "\n",
    "  -8x + 4 = 0\n",
    "  \n",
    "  x = 0.5\n",
    "  \n",
    "* Now lets put 0.5 in equation to find the maximum values\n",
    "\n",
    "  f(0,5) =  -4(0.5)^2 + 4(0.5) + 13 = -1 + 2 +13 = 14\n",
    "  \n",
    "* This functiona will have concave shape. So the maximum point is (0.5, 14)  \n",
    "\n",
    "* Reference: https://www.youtube.com/watch?v=lRAgottY8XU&list=PLjW9PIyfCennBOprV3CPoqMX8SW-qNlUa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039423,
     "end_time": "2022-11-08T11:13:35.781987",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.742564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Below is the output of a correlation matrix from your Exploratory data. Is using all the features in a model appropriate for predicting/inferencing Y?\n",
    "![](https://raw.githubusercontent.com/satishgunjal/images/master/Correlation_Matrix.PNG)\n",
    "    \n",
    "* We can see from above correlation matrix that there is high correlation(.98) between X1 and X2, also high correlation(.88) between X1 and X3, similarly there is high correlation(.75) between X2 and X3\n",
    "* All the variables are correlated to each other. In regression this would result in multicollinearity. We can try methods such as dimension reduction, feature selection, stepwise regression to choose the correct input variables for predictiong Y\n",
    "* Second part of question is - should we use all the variables for modeling?\n",
    "  - Using multicolnear feature in modeling doesnt help. We should remove all the multicolnear feature and keep unique feature so that explaining the model predictions also becomes easy.\n",
    "  - It will also make model less complex and we dont have to store many features.\n",
    "\n",
    "## Prediction vs Inference\n",
    "Inference and prediction are two often confused terms, perhaps in part because they are not mutually exclusive.\n",
    "![](https://storage.ning.com/topology/rest/1.0/file/get/4901412886?profile=original)\n",
    "Reference: https://www.datasciencecentral.com/profiles/blogs/inference-vs-prediction-in-one-picture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041873,
     "end_time": "2022-11-08T11:13:35.862695",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.820822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is stepwise regression?\n",
    "Stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some prespecified criterion.\n",
    "\n",
    "Stepwise regression is classified into backward and forward selection.\n",
    "* **Backward selection** starts with a full model, then step by step we reduce the regressor variables and find the model with the least RSS, largest R², or the least MSE. The variables to drop would be the ones with high p-values.\n",
    "* **Forward selection** starts with a null model, then step by step we increase the regressor variables until we can no longer improve the error performance of the model. We usually pick the model with the highest adjusted R²."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039881,
     "end_time": "2022-11-08T11:13:35.942385",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.902504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# You have two buckets - one of 3 liters and other of 5 liters. You are expected to mesure exactly 4 liters. How will you complete the task? Note: There is no thrid bucket.\n",
    " \n",
    " * Questions like this will test your out of the box thinking\n",
    " * Step1: Fill 5 lts bucket and  empty it in 3 ltr bucket. Now we are left with 2 ltr in 5 ltr bucket.\n",
    " * Step2: Empty 3 ltr bucket and pour the contents of 5 ltr bucket in 3 ltr bucket. Now our 5 ltr bucket is empty and 3 ltr bucket has 2 ltr content in it.\n",
    " * Now fill the 5 ltr bucket again. Remember that our 3 ltr bucket has 2 ltr content in it, so if we pour 1 ltr content from 5 ltr bucket to 3 ltr bucket we are left with 4 ltr content in 5 ltr bucket.\n",
    " * Reference: https://youtu.be/5JZsSNLXXuE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042197,
     "end_time": "2022-11-08T11:13:36.024463",
     "exception": false,
     "start_time": "2022-11-08T11:13:35.982266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lis the differences between supervised and unsupervised learning\n",
    "\n",
    "| Supervised learning | Unsupervised leanring\n",
    "|:- |:-\n",
    "Uses labeled data as input | Uses unlabeled data as input\n",
    "Supervised learning has feedback mechanism | Unsupervised learning has no feedback mechanism\n",
    "Common supervised learning algorithms are decision tree, logistic regression, support vector machine etc | K Means clustering, hierarchical clustering etc\n",
    "\n",
    "\n",
    "Reference: https://youtu.be/5JZsSNLXXuE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039311,
     "end_time": "2022-11-08T11:13:36.102711",
     "exception": false,
     "start_time": "2022-11-08T11:13:36.063400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain the steps in making decision tree?\n",
    "\n",
    "![](https://raw.githubusercontent.com/satishgunjal/images/master/Decision_Tree.png)\n",
    "\n",
    "\n",
    "Below are the common steps in decision tree algorithm\n",
    "* Take the entire data as input\n",
    "* At the root node decision tree selects feature to split the data in two major categories.\n",
    "* Different criteria will be used to split the data. We generally use 'entropy' or 'gini' in case of classification and 'mse' or 'mae' in case of regression problems.\n",
    "* Features are selected for spliting based on highest information gain.\n",
    "* After every split we get decision rules and sub trees.\n",
    "* This process will continue until every training example is grouped together or maxinum allowed tree depth is reached.\n",
    "* So at the end of decision tree we end up with leaf node. Which represent the class or a continuous value that we are trying predict\n",
    "* Reference: https://satishgunjal.com/decision_tree/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039213,
     "end_time": "2022-11-08T11:13:36.181408",
     "exception": false,
     "start_time": "2022-11-08T11:13:36.142195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How do you build random forest model?\n",
    "\n",
    "Ranodm forest is made up of multiple decision trees. Unlike decision tree random forest fits multiple decision trees on various sub samples of dataset and make the predictions by averaging the predictions from each tree. \n",
    "\n",
    "![](https://raw.githubusercontent.com/satishgunjal/images/master/Random_Forest.png)\n",
    "\n",
    "* Select few random sub sample from given dataset\n",
    "* Construct a decision tree for every sub sample and predict the result.\n",
    "* Perform the voting on prediction from each tree.\n",
    "* At the end select the most voted result as final prediction.\n",
    "* Reference: https://satishgunjal.com/random_forest/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039301,
     "end_time": "2022-11-08T11:13:36.260275",
     "exception": false,
     "start_time": "2022-11-08T11:13:36.220974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How do Random Forest handle missing data?\n",
    "Note that handling missing data is one of the advantages of Random Forest algorithm over Decision tree. Please refer below diagram where we have training data set of circle, square and triangle of color red, green and blue respectively. There are total 27 training examples.\n",
    "\n",
    "![](https://raw.githubusercontent.com/satishgunjal/images/master/Random_Forest.png)\n",
    "\n",
    "* Random forest will create three sub sample of 9 training examples each\n",
    "* Random forest algorithm will create three different decision tree for each sub sample\n",
    "* Notice that each tree uses different criteria to split the data\n",
    "* Now it is straight forward analysis for the algorithm to predict the shape of given figure if its shape and color is known. Let’s check the predictions of each tree for blue color triangle, (here shape input is missing)\n",
    "    * Tree 1 will predict: triangle\n",
    "    * Tree 2 will predict: square\n",
    "    * Tree 2 will predict: triangle \n",
    "    \n",
    "    Since the majority of voting is for triangle final prediction is ‘triangle shape’\n",
    "    \n",
    "* Now, lets check predictions for circle with no color defined (color attribute is missing here)\n",
    "    * Tree 1 will predict: triangle\n",
    "    * Tree 2 will predict: circle\n",
    "    * Tree 2 will predict: circle \n",
    "    \n",
    "    Since the majority of voting is for circle final prediction is ‘circle shape’\n",
    "\n",
    "Please note this is over simplified example, but you get an idea how multiple tree with different split criteria helps to handle missing features\n",
    "\n",
    "Reference: https://satishgunjal.com/random_forest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038582,
     "end_time": "2022-11-08T11:13:36.341038",
     "exception": false,
     "start_time": "2022-11-08T11:13:36.302456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is model overfitting? How can you avoid it?\n",
    "Overfitting occurs when your model learns too much from training data and isn't able to generalize the underlying information. When this happens, the model is able to describe training data very accurately but loses precision on every dataset it has not been trained on. Below images represent the overfitting linear and logistic regression models.\n",
    "\n",
    "![](https://raw.githubusercontent.com/satishgunjal/images/master/Overfitting.png)\n",
    "\n",
    "**How To Avoid Overfitting?**\n",
    "* Since overfitting algorithm captures the noise in data, reducing the number of features will help. We can manually select only important features or can use model selection algorithm for same\n",
    "* We can also use the ‘Regularization’ technique. It works well when we have lots of slightly useful features. Sklearn linear model(Ridge and LASSO) uses regularization parameter ‘alpha’ to control the size of the coefficients by imposing a penalty. \n",
    "* K-fold cross validation. In this technique we divide the training data in multiple batches and use each batch for training and testing the model.\n",
    "* Increasing the training data also helps to avoid overfitting.\n",
    "\n",
    "Reference: https://satishgunjal.com/underfitting_overfitting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03889,
     "end_time": "2022-11-08T11:13:36.418885",
     "exception": false,
     "start_time": "2022-11-08T11:13:36.379995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# There are 9 balls out of which one ball is heavy in weight and rest are of the same weight. In how many minimum weightings will you find the heavier ball?\n",
    "* You will need two weightings\n",
    "* Step1: Out of 9 balls, place three balls on each side (you will have three remaining balls)\n",
    "    * Scenario: Balance out\n",
    "    \n",
    "    In balance out scenario, the heaviest ball is definately part of three remaining balls. Out of the remaining three balls from step 1, take two balls and place one ball on each side- if they balance out then the left out ball will be the heavier ball. Otherwise, you will see it in the balance.\n",
    "    \n",
    "    * Scenario: Not balance out\n",
    "    \n",
    "    If the balls in step 1 do not balance out, that means heavier side has the heavier ball.\n",
    "* Reference: https://www.youtube.com/watch?v=5JZsSNLXXuE&list=PLwWVLyefnzgpWxe2WEPrmHqHzwHlyZw1U&index=2&t=1294s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038647,
     "end_time": "2022-11-08T11:13:36.496921",
     "exception": false,
     "start_time": "2022-11-08T11:13:36.458274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Difference between univariate, bivariate and multivariate analysis?\n",
    "\n",
    "* Univariate Analysis\n",
    "\n",
    "![Univariate_Analysis](https://raw.githubusercontent.com/satishgunjal/images/master/Univariate_Analysis.PNG)\n",
    "\n",
    "* Bivariate Analysis\n",
    "\n",
    "![Bivariate_Analysis](https://raw.githubusercontent.com/satishgunjal/images/master/Bivariate_Analysis.PNG)\n",
    "\n",
    "* Multivariate Analysis\n",
    "\n",
    "![Multivariate_Analysis](https://raw.githubusercontent.com/satishgunjal/images/master/Multivariate_Analysis.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038699,
     "end_time": "2022-11-08T11:13:36.574786",
     "exception": false,
     "start_time": "2022-11-08T11:13:36.536087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What are feature selection methods to select right variables?\n",
    "Feature selection is the process of reducing the number of input variables when developing a predictive model. There are two methods for feature selection. Filter method and wrapper methods. Best analogy for selecting features is bad data in bad answers out.\n",
    "\n",
    "### Filter Methods\n",
    "* Filter feature selection methods use statistical techniques to evaluate the relationship between each input variable and the target variable, and these scores are used as the basis to choose (filter) those input variables that will be used in the model.\n",
    "* These methods are faster and less computationally expensive than wrapper methods.\n",
    "\n",
    "#### Information Gain\n",
    "\n",
    "Information gain calculates the reduction in entropy from the transformation of a dataset. It can be used for feature selection by evaluating the Information gain of each variable in the context of the target variable.\n",
    "\n",
    "#### Chi-square Test\n",
    "The Chi-square test is used for categorical features in a dataset. We calculate Chi-square between each feature and the target and select the desired number of features with the best Chi-square scores. \n",
    "\n",
    "#### Correlation Coefficient\n",
    "Correlation is a measure of the linear relationship of 2 or more variables. Through correlation, we can predict one variable from the other. The logic behind using correlation for feature selection is that the good variables are highly correlated with the target. Furthermore, variables should be correlated with the target but should be uncorrelated among themselves.\n",
    "\n",
    "### Wrapper Methods\n",
    "* Wrapper feature selection methods create many models with different subsets of input features and select those features that result in the best performing model according to a performance metric.\n",
    "* These methods are unconcerned with the variable types, although they can be computationally expensive.\n",
    "* The wrapper methods usually result in better predictive accuracy than filter methods.\n",
    "\n",
    "#### Forward Feature Selection\n",
    "This is an iterative method wherein we start with the best performing variable against the target. Next, we select another variable that gives the best performance in combination with the first selected variable. This process continues until the preset criterion is achieved.\n",
    "\n",
    "#### Backward Feature Elimination\n",
    "This method works exactly opposite to the Forward Feature Selection method. Here, we start with all the features available and build a model. Next, we remove the variable from the model which gives the best evaluation measure value. This process is continued until the preset criterion is achieved.\n",
    "\n",
    "#### Exhaustive Feature Selection\n",
    "This is the most robust feature selection method covered so far. This is a brute-force evaluation of each feature subset. This means that it tries every possible combination of the variables and returns the best performing subset.\n",
    "\n",
    "Reference: https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050212,
     "end_time": "2022-11-08T11:13:36.678127",
     "exception": false,
     "start_time": "2022-11-08T11:13:36.627915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# In you choice of langauge: Write a program that prints the numbers from 1 to 50. But for multiples of three print \"Fizz\" instaed of the number and for the multiples of five print \"Buzz\". For the numbers which are multiples of both three and five print \"FizzBuzz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-11-08T11:13:36.771745Z",
     "iopub.status.busy": "2022-11-08T11:13:36.771008Z",
     "iopub.status.idle": "2022-11-08T11:13:36.783361Z",
     "shell.execute_reply": "2022-11-08T11:13:36.782609Z"
    },
    "papermill": {
     "duration": 0.056791,
     "end_time": "2022-11-08T11:13:36.783489",
     "exception": false,
     "start_time": "2022-11-08T11:13:36.726698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Fizz\n",
      "4\n",
      "Buzz\n",
      "Fizz\n",
      "7\n",
      "8\n",
      "Fizz\n",
      "Buzz\n",
      "11\n",
      "Fizz\n",
      "13\n",
      "14\n",
      "FizzBuzz\n",
      "16\n",
      "17\n",
      "Fizz\n",
      "19\n",
      "Buzz\n",
      "Fizz\n",
      "22\n",
      "23\n",
      "Fizz\n",
      "Buzz\n",
      "26\n",
      "Fizz\n",
      "28\n",
      "29\n",
      "FizzBuzz\n",
      "31\n",
      "32\n",
      "Fizz\n",
      "34\n",
      "Buzz\n",
      "Fizz\n",
      "37\n",
      "38\n",
      "Fizz\n",
      "Buzz\n",
      "41\n",
      "Fizz\n",
      "43\n",
      "44\n",
      "FizzBuzz\n",
      "46\n",
      "47\n",
      "Fizz\n",
      "49\n",
      "Buzz\n"
     ]
    }
   ],
   "source": [
    "# The continue statement rejects all the remaining statements in the current iteration of the loop and \n",
    "# moves the control back to the top of the loop.\n",
    "for i in range(1, 51):    \n",
    "    if (i%3 == 0 and i%5 == 0):\n",
    "        print(\"FizzBuzz\")\n",
    "        continue\n",
    "    if i%3 == 0:\n",
    "        print(\"Fizz\")\n",
    "        continue\n",
    "    if i%5 == 0:\n",
    "        print(\"Buzz\")\n",
    "        continue\n",
    "    print(i)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042234,
     "end_time": "2022-11-08T11:13:36.866219",
     "exception": false,
     "start_time": "2022-11-08T11:13:36.823985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# You are given a dataset consisting of variables having more than 30% missing values? How will you deal with them?\n",
    "* There are multiple ways to handle missing values in the data\n",
    "* If dataset is huge we can simply remove the rows containing the missing data\n",
    "* If dataset is small then we have to impute the missing values. There are multiple ways to impute the missing values. In case of categorical data we may use the most common values and in case numerical data we can use mean, median etc.\n",
    "* Reference: https://youtu.be/5JZsSNLXXuE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041743,
     "end_time": "2022-11-08T11:13:36.948265",
     "exception": false,
     "start_time": "2022-11-08T11:13:36.906522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# For the given point how will you caluclate the Euclidean distance, in Python?\n",
    "\n",
    "Euclidean distance is calculated as the square root of the sum of the squared differences between the two vectors.\n",
    "\n",
    "![](https://predictivehacks.com/wp-content/uploads/2020/08/2d_euclidean_distance_illustration.png)\n",
    "Reference: https://predictivehacks.com/tip-how-to-define-your-distance-function-for-hierarchical-clustering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-08T11:13:37.036043Z",
     "iopub.status.busy": "2022-11-08T11:13:37.035336Z",
     "iopub.status.idle": "2022-11-08T11:13:37.038708Z",
     "shell.execute_reply": "2022-11-08T11:13:37.039245Z"
    },
    "papermill": {
     "duration": 0.050214,
     "end_time": "2022-11-08T11:13:37.039414",
     "exception": false,
     "start_time": "2022-11-08T11:13:36.989200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.242640687119285\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# define the points\n",
    "p1 = [6,5]\n",
    "p2 = [3, 2]\n",
    "\n",
    "euclidean_distance = math.sqrt( (p1[0]-p2[0])**2 + (p1[1]-p2[1])**2 )\n",
    "print(euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039666,
     "end_time": "2022-11-08T11:13:37.120663",
     "exception": false,
     "start_time": "2022-11-08T11:13:37.080997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is the angle between the hour and minute hands of clock when the time is half past six?\n",
    "![Clock_Puzzle](https://raw.githubusercontent.com/satishgunjal/images/master/Clock_Puzzle.PNG)\n",
    "Reference: https://youtu.be/5JZsSNLXXuE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040161,
     "end_time": "2022-11-08T11:13:37.201319",
     "exception": false,
     "start_time": "2022-11-08T11:13:37.161158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How should you maintain your deployed model?\n",
    "\n",
    "### Monitor\n",
    "Constant monitoring of all the models is needed to determine the performance accuracy of the models\n",
    "\n",
    "### Evaluate\n",
    "Evaluation metric of the current model is calculated to determine if new algorithm is needed.\n",
    "\n",
    "### Compare\n",
    "The new models are compared against each other to determine which model performs the best.\n",
    "\n",
    "###  Rebuild\n",
    "The best performing model is re-built on the current set of data.\n",
    "\n",
    "Reference: https://youtu.be/5JZsSNLXXuE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041754,
     "end_time": "2022-11-08T11:13:37.283739",
     "exception": false,
     "start_time": "2022-11-08T11:13:37.241985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What are recommender systems?\n",
    "* The purpose of a recommender system is to suggest relevant items or services to users.\n",
    "* Two major categories of recommender systems are collaboarative filtering and cotent based filtering methods\n",
    "\n",
    "### Collaborative Filtering\n",
    "* It is based on the past interactions recorded between users and items in order to produce new recommendations.\n",
    "* e.g. Music service recommends track that are often played by other users with similar interests\n",
    "\n",
    "### Content Based Filtering\n",
    "* Unlike collaborative methods that only rely on the user-item interactions, content based approaches use additional information about the content consumed by the user to produce new recommendations\n",
    "* e.g. Music service recommends new song based on properties of the songs user listens to.\n",
    "\n",
    "Reference: https://youtu.be/5JZsSNLXXuE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042505,
     "end_time": "2022-11-08T11:13:37.366834",
     "exception": false,
     "start_time": "2022-11-08T11:13:37.324329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 'People who bought this, also bought...'recommendations seen on Amazon is a result of which algorithm?\n",
    "\n",
    "* Its done by recommendation system using collaborative filtering approach.\n",
    "* In case of collaborative filtering past interactions recorded between users and items are used to produce new recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041971,
     "end_time": "2022-11-08T11:13:37.449687",
     "exception": false,
     "start_time": "2022-11-08T11:13:37.407716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# If it rains on saturday with probability 0.6, and it rains on sunday with probability 0.2, what is the probability that it rains this weekend?\n",
    "* Since we know the probability of rain on Saturday and Sunday, the probability of raining on Weekend is combination of both of these events.\n",
    "* Trick here is to know the probability of not raining on Saturday and Sunday.\n",
    "* If we subtract the intersection(∩) of both the events of not raining on Saturday and Sunday from total probability then we get the probability of raining on weekend.\n",
    " ```\n",
    " = Total probability - (Probability that it will not rain on Saturday) ∩ (Probability that it will not rain on Sunday)\n",
    " = 1 - (1 - 0.6)*(1 - 0.2)\n",
    " = 0.68\n",
    " ```\n",
    " * Reference: https://youtu.be/5JZsSNLXXuE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041061,
     "end_time": "2022-11-08T11:13:37.531418",
     "exception": false,
     "start_time": "2022-11-08T11:13:37.490357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How can you select K for K-Means?\n",
    "There are two ways to select the number of clusters in case K-Means clustering algorithm\n",
    "\n",
    "### Visualization\n",
    "* To find the number of clusters manually by data visualization is one of the most common method. \n",
    "* Domain knowledge and proper understanding of given data also help to make more informed decisions. \n",
    "* Since its manual exercise there is always a scope for ambiguous observations, in such cases we can also use ‘Elbow Method’\n",
    "\n",
    "### Elbow Method\n",
    "* In Elbow method we run the K-Means algorithm multiple times over a loop, with an increasing number of cluster choice(say from 1 to 10) and then plotting a clustering score as a function of the number of clusters. \n",
    "* Clustering score is nothing but sum of squared distances of samples to their closest cluster center. \n",
    "* Elbow is the point on the plot where clustering score (distortion) slows down, and the value of cluster at that point gives us the optimum number of clusters to have. \n",
    "* But sometimes we don’t get clear elbow point on the plot, in such cases its very hard to finalize the number of clusters.\n",
    "\n",
    "![K_Means_Elbow_Method](https://raw.githubusercontent.com/satishgunjal/images/master/K_Means_Elbow_Method.png)\n",
    "Reference: https://satishgunjal.com/kmeans/#5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041578,
     "end_time": "2022-11-08T11:13:37.613865",
     "exception": false,
     "start_time": "2022-11-08T11:13:37.572287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain dimensionality reduction, and its benefits?\n",
    "* Dimensionality reduction referes to the process of converting a set of data having vast dimensions into data with lesser dimensions(features) to convey similar information concisely.\n",
    "* It helps in data compressing and reducing the storage space\n",
    "* It reduces computation time as less dimensions lead to less computing\n",
    "* It removes redundant features. E.g. There is no point in storing value in two different units\n",
    "* Reference: https://youtu.be/5JZsSNLXXuE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040902,
     "end_time": "2022-11-08T11:13:37.695598",
     "exception": false,
     "start_time": "2022-11-08T11:13:37.654696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How can you say that the time series data is stationary?\n",
    "For accurate analysis and forecasting trend and seasonality is removed from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time.\n",
    "\n",
    "![Stationarity](https://raw.githubusercontent.com/satishgunjal/images/master/Stationarity.png)\n",
    "Reference: https://satishgunjal.com/time_series/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039777,
     "end_time": "2022-11-08T11:13:37.778779",
     "exception": false,
     "start_time": "2022-11-08T11:13:37.739002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How can you calculate the accuracy using confusion matrix?\n",
    "Accuracy = (True Positive + true Negative) / Total Obervations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.045266,
     "end_time": "2022-11-08T11:13:37.864310",
     "exception": false,
     "start_time": "2022-11-08T11:13:37.819044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Write the equations for the precision and recall?\n",
    "\n",
    "Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "Recall = True Positive /(Total Positive + False Negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046744,
     "end_time": "2022-11-08T11:13:37.956910",
     "exception": false,
     "start_time": "2022-11-08T11:13:37.910166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# If a drawer containes 12 red socks, 16 blue socks, and 20 white socks, how many must you pull out to be sure of having a amcthing pair?\n",
    "* There are three colors of socks- Red, Blue and White. No of socks is irrelevant here.\n",
    "* Suppose in our first pull we picked Red color sock\n",
    "* In second pull we picked Blue color sock\n",
    "* And in third pull we picked White color sock.\n",
    "* Now in our fourth pull, if we pick any color, match is guaranteed!! So the answer is 4!\n",
    "* Reference: https://youtu.be/5JZsSNLXXuE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042348,
     "end_time": "2022-11-08T11:13:38.044527",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.002179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Write a SQL query to list all orders with customer information\n",
    "![SQL_Join](https://raw.githubusercontent.com/satishgunjal/images/master/SQL_Join.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041256,
     "end_time": "2022-11-08T11:13:38.126519",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.085263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Which of the following machine learning algorithm can be used for imputing missing values of both categorical and continuos variables?\n",
    "\n",
    "```\n",
    "- K-means clustering\n",
    "- Linear regression\n",
    "- K-NN\n",
    "- Decision tress\n",
    "```\n",
    "\n",
    "Using KNN we can compute the missing variable value by using the nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04023,
     "end_time": "2022-11-08T11:13:38.208558",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.168328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Given a box of matches and two ropes, not necessarily identical, measure a period of 45 minutes? Note: Ropes are not uniform in natire and rope takes exactly 60 minutes to completly burn out\n",
    "\n",
    "* We have two ropes A and B\n",
    "* Ligt A from both the end and B from one end\n",
    "* When A finished burning we know that 30 minutes have elapsed and B has 30 minutes remaining\n",
    "* Now light the other end of B also, it will now burnout in 15 minutes\n",
    "* This we got 30 + 15 = 45 minutes\n",
    "* Reference: https://youtu.be/5JZsSNLXXuE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040269,
     "end_time": "2022-11-08T11:13:38.289066",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.248797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# After studying the behaviour of population, you have identified four specific individual types who are valueable to your study. You would like find all users who are most similar to each indivdual type. Which algorithm is most approprate for this study?\n",
    "```\n",
    "- K-means clstering\n",
    "- Linear regression\n",
    "- Associate rules\n",
    "- Decision tress\n",
    "```\n",
    "\n",
    "Answer is : K-means clustering\n",
    "\n",
    "Reference: https://youtu.be/5JZsSNLXXuE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042087,
     "end_time": "2022-11-08T11:13:38.372173",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.330086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Your organization has a website where visitors randomly receive one of the two coupons. It is also possible that visitors to the website will not receive the coupon. You have been asked to determine if offering a coupon to the visitors to your website has any impact on their purchase decision. Which analysis method should you use?\n",
    "```\n",
    "- One Way ANOVA\n",
    "- K-means clsutering\n",
    "- Assiciation rules\n",
    "- Student Test\n",
    "```\n",
    "\n",
    "Answer: One Way ANOVA\n",
    "\n",
    "Reference: https://youtu.be/5JZsSNLXXuE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041601,
     "end_time": "2022-11-08T11:13:38.456887",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.415286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain Principal Componenet Analysis?\n",
    "* Principal Component Analysis (PCA) is dimensionality reduction method, that is used to reduce dimensionality of large data sets, by transforming large set of variables into a smaller one that still contains most of the information in large set.\n",
    "* Principal component analysis is a technique for **feature extraction** — so it combines our input variables in a specific way, then we can drop the “**least important**” variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the “new” variables after PCA are all **independent** of one another\n",
    "* Reducing the number of the variables of the datset naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity.\n",
    "* By reducing the dimension of your feature space, you have fewer relationships between variables to consider and you are less likely to overfit your model\n",
    "\n",
    "### When should I use PCA?\n",
    "* Do you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration?\n",
    "* Do you want to ensure your variables are independent of one another?\n",
    "* Are you comfortable making your independent variables less interpretable?\n",
    "\n",
    "* Reference: https://builtin.com/data-science/step-step-explanation-principal-component-analysis, https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041262,
     "end_time": "2022-11-08T11:13:38.539175",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.497913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain feature scaling, normalization, standardization\n",
    "* Feature scaling is one of the most important data preprocessing step in machine learning\n",
    "* If we are changing the range of the features then its called 'scaling' and if we are changing the distribution of the features then its called 'normalization/standardization'\n",
    "\n",
    "## Scaling\n",
    "* This means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1. By scaling your variables, you can help compare different variables on equal footing.\n",
    "* Scaling is required in case of distance based algorithms like support vector machines (SVM) or k-nearest neighbors (KNN).\n",
    "* For example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices, methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n",
    "![](https://www.kaggleusercontent.com/kf/82374396/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..nCt8vTYDm1BgNMSpw4VoLA.t89cT2or10R1YfcqZaawXw-PaDt8M2-W0ky6F_A1yzOlkKKzv6-b0m7lPo5lp4UxyhTw3000J11OLydZaC_7efqISEudvrlCwGqL4jqTzY7FpYecB2OB9I-QpWG5K5_J9lnGk6YZLZSxKP4uDjhELSJ9ctc45zdEo0acGp_9VkwgOBiBVZbDHOBBi7Hv2Rjl2s5DWPwi1eUliOfN4AVPnQ2976-kpxm9Lh86hhVdaxWGfN8MHQWfIBI14BopXKj-lDaRSmQTHYiCYPpjmBKsvi3aNaH1HZXuBVdomNFAt256NdrYnqj-v--K_0ETQlnHE8Bz4rmvEsbLKs8jWygsSpOUy7jUG6BYN1lozPLzp2iia8_HOkzZ11SYv_qOkxSbZ-kZgT5NbFOY7jP_sPQUW5JQAohZHCCwgno0LjVyq4B_y-fUDVJO5QI0WE5vzm2Ahfe9-iv54QID1wR7djVxDv97I1ufeHl4Q-mckMwxQc9_n6BvuIwE9C3G040ZdGCQnMmyu0g-eSUwXUifY6P3NrsgGGGz7-tpomeh-Wv7LH7WSTFyHv1k1G4oXxhd--5yOPu5p23Vl8hS7JT_yE9jSIsJJFO0DRuqQX713zHXS2-WVSV8473Q5QBDZAMfMU9WfLihTiukVMFRiH1x27ax1cWrFai5BysTyyojVIx6Q5k.Um4UMdE37yBz5JubK-ZJAg/__results___files/__results___4_0.png)\n",
    "* Notice that the shape of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1. Here we have used min-max scaling\n",
    "\n",
    "## Normalization\n",
    "* Normalization is a more radical transformation, it changes data distribution to 'normal distribution'\n",
    "> Normal distribution: Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n",
    "* Some machine learning algorithms assumes that data should be normally distributed like linear discriminant analysis (LDA) and Gaussian naive Bayes.\n",
    "\n",
    "![](https://www.kaggleusercontent.com/kf/82374396/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..nCt8vTYDm1BgNMSpw4VoLA.t89cT2or10R1YfcqZaawXw-PaDt8M2-W0ky6F_A1yzOlkKKzv6-b0m7lPo5lp4UxyhTw3000J11OLydZaC_7efqISEudvrlCwGqL4jqTzY7FpYecB2OB9I-QpWG5K5_J9lnGk6YZLZSxKP4uDjhELSJ9ctc45zdEo0acGp_9VkwgOBiBVZbDHOBBi7Hv2Rjl2s5DWPwi1eUliOfN4AVPnQ2976-kpxm9Lh86hhVdaxWGfN8MHQWfIBI14BopXKj-lDaRSmQTHYiCYPpjmBKsvi3aNaH1HZXuBVdomNFAt256NdrYnqj-v--K_0ETQlnHE8Bz4rmvEsbLKs8jWygsSpOUy7jUG6BYN1lozPLzp2iia8_HOkzZ11SYv_qOkxSbZ-kZgT5NbFOY7jP_sPQUW5JQAohZHCCwgno0LjVyq4B_y-fUDVJO5QI0WE5vzm2Ahfe9-iv54QID1wR7djVxDv97I1ufeHl4Q-mckMwxQc9_n6BvuIwE9C3G040ZdGCQnMmyu0g-eSUwXUifY6P3NrsgGGGz7-tpomeh-Wv7LH7WSTFyHv1k1G4oXxhd--5yOPu5p23Vl8hS7JT_yE9jSIsJJFO0DRuqQX713zHXS2-WVSV8473Q5QBDZAMfMU9WfLihTiukVMFRiH1x27ax1cWrFai5BysTyyojVIx6Q5k.Um4UMdE37yBz5JubK-ZJAg/__results___files/__results___6_0.png)\n",
    "* Notice that the shape of our data has changed. Before normalizing it was almost L-shaped. But after normalizing it looks more like the outline of a bell (hence \"bell curve\"). Here we have used Box-Cox Transformation.\n",
    "\n",
    "## Standardization\n",
    "* Standardization typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance). For most applications standardization is recommended.\n",
    "* Scikit-Learn provides a transformer called StandardScaler for standardization.\n",
    "\n",
    "* Reference: https://www.kaggle.com/code/alexisbcook/scaling-and-normalization/tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039833,
     "end_time": "2022-11-08T11:13:38.620258",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.580425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Difference between standardisation and normalization?\n",
    "* Normalization typically means rescales the values into a range of [0,1]. This might be useful in some cases where all parameters need to have the same positive scale. However, the outliers from the data set are lost.\n",
    "* Standardization typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance). For most applications standardization is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039754,
     "end_time": "2022-11-08T11:13:38.700469",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.660715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is meant by Data Leakage?\n",
    "* Data Leakage is the scenario where the Machine Learning Model is already aware of some part of test data after training.This causes the problem of overfitting.\n",
    "* In Machine learning, Data Leakage refers to a mistake that is made by the creator of a machine learning model in which they accidentally share the information between the test and training data sets.\n",
    "* Data leakage is a serious and widespread problem in data mining and machine learning which needs to be handled well to obtain a robust and generalized predictive model.\n",
    "\n",
    "**Examples of data leakage**\n",
    "* The most obvious and easy-to-understand cause of data leakage is to include the target variable as a feature. What happens is that after including the target variable as a feature, our purpose of prediction got destroyed. This is likely to be done by mistake but while modelling any ML model, you have to make sure that the target variable is differentiated from the set of features.\n",
    "* Another common cause of data leakage is to include test data with training data.\n",
    "\n",
    "> Above two cases are not very likely to occur because they can easily be spotted while doing the modelling. Below are few data leakage examples that are hard to troubleshoot.\n",
    "\n",
    "* Presence of Giveaway features\n",
    "    * Let’s we are working on a problem statement in which we have to build a model that predicts a certain medical condition. If we have a feature that indicates whether a patient had a surgery related to that medical condition, then it causes data leakage and we should never be included that as a feature in the training data. The indication of surgery is highly predictive of the medical condition and would probably not be available in all cases. If we already know that a patient had a surgery related to a medical condition, then we may not even require a predictive model to start with.\n",
    "    * Let’s we are working on a problem statement in which we have to build a model that predicts if a user will stay on a website. Including features that expose the information about future visits will cause the problem of data leakage. So, we have to use only features about the current session because information about the future sessions is not generally available after we deployed our model.\n",
    "\n",
    "\n",
    "* Leakage during Data preprocessing\n",
    "\n",
    "    * While solving a Machine learning problem statement, firstly we do the data cleaning and preprocessing which involves the following steps:\n",
    "\n",
    "       * Evaluating the parameters for normalizing or rescaling features\n",
    "       * Finding the minimum and maximum values of a particular feature\n",
    "       * Normalize the particular feature in our dataset\n",
    "       * Removing the outliers\n",
    "       * Fill or completely remove the missing data in our dataset\n",
    "\n",
    "    * The above-described steps should be done using only the training set. If we use the entire dataset to perform these operations, data leakage may occur. \n",
    "    * Applying preprocessing techniques to the entire dataset will cause the model to learn not only the training set but also the test set. As we all know that the test set should be new and previously unseen for any model.\n",
    "\n",
    "* Reference: https://www.analyticsvidhya.com/blog/2021/07/data-leakage-and-its-effect-on-the-performance-of-an-ml-model/#:~:text=How%20does%20it%20exactly%20happen,“leakage”%20instead%20of%20cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039618,
     "end_time": "2022-11-08T11:13:38.780251",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.740633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How to detect Data Leakage?\n",
    "* Results are too good too true\n",
    "    * In general, if we see that the model which we build is too good to be true (i.,e gives predicted and actual output the same), then we should get suspicious and data leakage cannot be ruled out.\n",
    "    * At that time, the model might be somehow memorizing the relations between feature and target instead of learning and generalizing it for the unseen data. \n",
    "    * So, it is advised that before the testing, the prior documented results are weighed against the expected results.\n",
    "* Using EDA\n",
    "    * While doing the Exploratory Data Analysis (EDA), we may detect features that are very highly correlated with the target variable. Of course, some features are more correlated than others but a surprisingly high correlation needs to be checked and handled carefully. \n",
    "    * We should pay close attention to those features. So, with the help of EDA, we can examine the raw data through statistical and visualization tools.\n",
    "* High weight features\n",
    "    * After the completion of the model training, if features are having very high weights, then we should pay close attention. Those features might be leaky.\n",
    "    \n",
    "* Reference: https://www.analyticsvidhya.com/blog/2021/07/data-leakage-and-its-effect-on-the-performance-of-an-ml-model/#:~:text=How%20does%20it%20exactly%20happen,“leakage”%20instead%20of%20cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040795,
     "end_time": "2022-11-08T11:13:38.861144",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.820349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How to fix the problem of Data Leakage?\n",
    "The main culprit behind this is the way we split our dataset and when. The following steps can prove to be very crucial in preventing data leakage:\n",
    "* Select the features such a way that they do not contain information about the target variable, which is not naturally available at the time of prediction.\n",
    "* Create a Separate Validation Set\n",
    "    * To minimize or avoid the problem of data leakage, we should try to set aside a validation set in addition to training and test sets if possible. \n",
    "    * The purpose of the validation set is to mimic the real-life scenario and can be used as a final step. \n",
    "    * By doing this type of activity, we will identify if there is any possible case of overfitting which in turn can act as a caution warning against deploying models that are expected to underperform in the production environment.\n",
    "* Apply Data preprocessing Separately to both Train and Test subsets\n",
    "    * While dealing with neural networks, it is a common practice that we normalize our input data firstly before feeding it into the model. \n",
    "    * Generally, data normalization is done by dividing the data by its mean value. More often than not, this normalization is applied to the overall data set, which influences the training set from the information of the test set and eventually it results in data leakage. \n",
    "    * Hence, to avoid data leakage, we have to apply any normalization technique separately to both training and test subsets.\n",
    "* Problem with the Time-Series Type of data\n",
    "    * When dealing with time-series data, we should pay more attention to data leakage. For example, if we somehow use data from the future when doing computations for current features or predictions, it is highly likely to end up with a leaked model. \n",
    "    * It generally happens when the data is randomly split into train and test subsets. \n",
    "    * So, when working with time-series data, we put a cutoff value on time which might be very useful, as it prevents us from getting any information after the time of prediction.\n",
    "    \n",
    "* Reference: https://www.analyticsvidhya.com/blog/2021/07/data-leakage-and-its-effect-on-the-performance-of-an-ml-model/#:~:text=How%20does%20it%20exactly%20happen,“leakage”%20instead%20of%20cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040297,
     "end_time": "2022-11-08T11:13:38.941822",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.901525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is selection bias?\n",
    "* Selection bias is the bias introduced by the selection of individuals, groups, or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the **selection effect**.\n",
    "* **Sampling bias** is usually classified as a subtype of selection bias, sampling bias is a bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.\n",
    "* Due to sampling bias, the probability distribution in the collected dataset deviates from its true natural distribution, which may affect ML models performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043774,
     "end_time": "2022-11-08T11:13:39.028191",
     "exception": false,
     "start_time": "2022-11-08T11:13:38.984417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Difference between supervised and unsupervised learning\n",
    "|Supervised|Unsupervised|\n",
    "|:-|:-|\n",
    "|Used for prediction|Used for analysis|\n",
    "|Labelled input data|Unlabelled input data|\n",
    "|Data need to be splitted into train/validation/test sets|No split required|\n",
    "|Used in Classification and Regression|Used for clustering, dimension reduction & density estimation|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040848,
     "end_time": "2022-11-08T11:13:39.111406",
     "exception": false,
     "start_time": "2022-11-08T11:13:39.070558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain normal distribution of data\n",
    "Data can be distributed (spread out) in different ways,\n",
    "* It can be spread out more on the left\n",
    "\n",
    "![](https://www.mathsisfun.com/data/images/normal-distribution-skew-left.gif)\n",
    "* More on the right\n",
    "\n",
    "![](https://www.mathsisfun.com/data/images/normal-distribution-skew-right.gif)\n",
    "* It can be all jumbled up\n",
    "\n",
    "![](https://www.mathsisfun.com/data/images/normal-distribution-random.gif)\n",
    "\n",
    "* But there are many cases where the data tends to be around a central value with no bias left or right, and it gets close to a \"Normal Distribution\" like this:\n",
    "\n",
    "![](https://www.mathsisfun.com/data/images/normal-distribution-1.svg)\n",
    "\n",
    "* The Normal Distribution has:\n",
    "    - mean = median = mode\n",
    "    - symmetry about the center\n",
    "    - 50% of values less than the mean and 50% greater than the mean\n",
    "\n",
    "Reference: https://www.mathsisfun.com/data/standard-normal-distribution.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039859,
     "end_time": "2022-11-08T11:13:39.191754",
     "exception": false,
     "start_time": "2022-11-08T11:13:39.151895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Expalin covariance and correlation\n",
    "* Covariance and Correlation are two mathematical concepts which are commonly used in the field of probability and statistics. Both concepts describe the relationship between two variables.\n",
    "* “Covariance” indicates the direction of the linear relationship between variables. “Correlation” on the other hand measures both the strength and direction of the linear relationship between two variables.\n",
    "* In case of High correlation, two sets of data are strongly linked together \n",
    "    - Correlation is Positive when the values increase together, and\n",
    "    - Correlation is Negative when one value decreases as the other increases\n",
    "    \n",
    "    \n",
    "   ![](https://www.mathsisfun.com/data/images/correlation-examples.svg)\n",
    "   \n",
    "Reference: https://www.mathsisfun.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040087,
     "end_time": "2022-11-08T11:13:39.272805",
     "exception": false,
     "start_time": "2022-11-08T11:13:39.232718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is regularization. Why it is usefull?\n",
    "* Regularization is the process of adding tunning parameter(penalty term) to a model to induce smoothness in order to prevent overfitting.\n",
    "* The tunning parameter controls the excessively fluctuating function in such a way that coefficients dont take extreame values.\n",
    "* There are two types of regularization as follows:\n",
    "    - L1 Regularization or Lasso Regularization.\n",
    "    L1 Regularization or Lasso Regularization adds a penalty to the error function. The penalty is the sum of the absolute values of weights.\n",
    "    - L2 Regularization or Ridge Regularization.\n",
    "    L2 Regularization or Ridge Regularization also adds a penalty to the error function. But the penalty here is the sum of the squared values of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040579,
     "end_time": "2022-11-08T11:13:39.354351",
     "exception": false,
     "start_time": "2022-11-08T11:13:39.313772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What are confouding varaiables?\n",
    "* In statistics, confounder is a variable that influences both the dependent variable and independent avriable.\n",
    "* If you are researeching whether a lack of exercise leads to weight gain. In this case 'lack of exercise' is independent variable and 'weight gain' is dependent variable. A confounding varaible in this case would be 'age' which affect both of these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041662,
     "end_time": "2022-11-08T11:13:39.439463",
     "exception": false,
     "start_time": "2022-11-08T11:13:39.397801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain ROC curve and AUC\n",
    "* Receiver Operating Characteristics(ROC) curve is very usefull tool for predicting the probability of binary classifier\n",
    "* It is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0\n",
    "    - False positive rate = FP / (FP + TN)\n",
    "    - True positive rate (Sensitivity) = TP / (TP + FN\n",
    "    \n",
    "For more detailed explaination please refer. https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043442,
     "end_time": "2022-11-08T11:13:39.525922",
     "exception": false,
     "start_time": "2022-11-08T11:13:39.482480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain Precision-Recall Curve\n",
    "\n",
    "   ![Precision-Recall Curve](https://raw.githubusercontent.com/satishgunjal/images/6637418f75e7387d8185cc57fde5dd72bc49b7cc/Precision%20Recall%20Curve.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04072,
     "end_time": "2022-11-08T11:13:39.607774",
     "exception": false,
     "start_time": "2022-11-08T11:13:39.567054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is TF-IDF?\n",
    "* TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents\n",
    "* It is used in information retrieval and text mining\n",
    "* TF-IDF (term frequency-inverse document frequency) was invented for document search and information retrieval. It works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don’t mean much to that document in particular.\n",
    "* However, if the word Bug appears many times in a document, while not appearing many times in others, it probably means that it’s very relevant. For example, if what we’re doing is trying to find out which topics some NPS responses belong to, the word Bug would probably end up being tied to the topic Reliability, since most responses containing that word would be about that topic.\n",
    "* Reference: https://monkeylearn.com/blog/what-is-tf-idf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041125,
     "end_time": "2022-11-08T11:13:39.690558",
     "exception": false,
     "start_time": "2022-11-08T11:13:39.649433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Python or R- which one would you prefer for text analytics?\n",
    "We will prefer python for following reasons\n",
    "* We can use pandas library which has easy to use data structures and  high performance data analysis tools\n",
    "* R is more suitable for ML than text analytics\n",
    "* Python is faster for all types of text analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041872,
     "end_time": "2022-11-08T11:13:39.774071",
     "exception": false,
     "start_time": "2022-11-08T11:13:39.732199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What are Eigenvectors and Eigenvalues?\n",
    "* Eigenvectors are used for understanding the linear transformations.\n",
    "* In data analysis, we usually calculate the eigenvectors for correlation or covariance matrix\n",
    "* Eigenvectors are the directions along which a particular linear transformation acts by flipping, compressing or stretching.\n",
    "* Eigenvalues can be referred to as the strength of the transaformation in direction of eigenvector or the factor by which compression occurs.\n",
    "* Refer. https://www.youtube.com/watch?v=glaiP222JWA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04077,
     "end_time": "2022-11-08T11:13:39.856733",
     "exception": false,
     "start_time": "2022-11-08T11:13:39.815963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain flase positive an false negative with examples.\n",
    "* A false positive is where you receive a positive result for a test, when you should have received a negative results. It’s sometimes called a “false alarm” or “false positive error.” It’s usually used in the medical field, but it can also apply to other arenas (like software testing). \n",
    "* Some examples of false positives:\n",
    "    - A pregnancy test is positive, when in fact you aren’t pregnant.\n",
    "    - A cancer screening test comes back positive, but you don’t have the disease.\n",
    "    - A prenatal test comes back positive for Down’s Syndrome, when your fetus does not have the disorder(1).\n",
    "    - Virus software on your computer incorrectly identifies a harmless program as a malicious one.\n",
    "* False positives can be worrisome, especially when it comes to medical tests. Researchers are consistently trying to identify reasons for false positives in order to make tests more sensitive.\n",
    "* A related concept is a false negative, where you receive a negative result when you should have received a positive one. For example, a pregnancy test may come back negative even though you are in fact pregnant.\n",
    "* Reference: https://www.statisticshowto.com/false-positive-definition-and-examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040037,
     "end_time": "2022-11-08T11:13:39.937809",
     "exception": false,
     "start_time": "2022-11-08T11:13:39.897772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain the scenario where both false positive and false negative are equally important\n",
    "* In banking industry giving loans is the primary source of making money, but at the same time bank can make profit if the repayment rate is good.\n",
    "* Bank always try not loose good customers and avoid bad ones. In this case false positive and false negative becomes very important to measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041301,
     "end_time": "2022-11-08T11:13:40.019443",
     "exception": false,
     "start_time": "2022-11-08T11:13:39.978142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Why feature scalling is required in Gradient Descent Based Algorithms\n",
    "* Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Take a look at the formula for gradient descent below:\n",
    "\n",
    "    ![Gradient descent formula](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/gradient-descent.png)\n",
    "\n",
    "* The presence of feature value X in the formula will affect the step size of the gradient descent. \n",
    "* The difference in ranges of features will cause different step sizes for each feature. \n",
    "* To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model.\n",
    "* Having features on a similar scale can help the gradient descent converge more quickly towards the minima.\n",
    "* Reference: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039705,
     "end_time": "2022-11-08T11:13:40.099313",
     "exception": false,
     "start_time": "2022-11-08T11:13:40.059608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Why feature scalling is required in distance Based Algorithms\n",
    "* Distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.\n",
    "* For example, let’s say we have data containing high school CGPA scores of students (ranging from 0 to 5) and their future incomes (in thousands Rupees):\n",
    "\n",
    "    ![Feature scaling: Unscaled Knn example](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/knn_ex.png)\n",
    "\n",
    "* Since both the features have different scales, there is a chance that higher weightage is given to features with higher magnitude. This will impact the performance of the machine learning algorithm and obviously, we do not want our algorithm to be biassed towards one feature.\n",
    "* Therefore, we scale our data before employing a distance based algorithm so that all the features contribute equally to the result.\n",
    "\n",
    "    ![Feature scaling: Scaled Knn example](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/knn_ex_scaled.png)\n",
    "\n",
    "* The effect of scaling is conspicuous(clearly visible) when we compare the Euclidean distance between data points for students A and B, and between B and C, before and after scaling as shown below:\n",
    "\n",
    "    Distance AB before scaling => ![Euclidean distance](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/eq1.gif)\n",
    "\n",
    "    Distance BC before scaling => ![Euclidean distance](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/eq2.gif)\n",
    "\n",
    "    Distance AB after scaling => ![Euclidean distance](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/eq3.gif)\n",
    "\n",
    "    Distance BC after scaling => ![Euclidean distance](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/eq4.gif)\n",
    "\n",
    "* Scaling has brought both the features into the picture and the distances are now more comparable than they were before we applied scaling.\n",
    "* Reference: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039631,
     "end_time": "2022-11-08T11:13:40.179404",
     "exception": false,
     "start_time": "2022-11-08T11:13:40.139773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Why feature scaling not required in tree based algorithms\n",
    "* Tree-based algorithms, on the other hand, are fairly insensitive to the scale of the features. Think about it, a decision tree is only splitting a node based on a single feature. The decision tree splits a node on a feature that increases the homogeneity of the node. This split on a feature is not influenced by other features.\n",
    "* So, there is virtually no effect of the remaining features on the split. This is what makes them invariant to the scale of the features!\n",
    "* Reference: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039643,
     "end_time": "2022-11-08T11:13:40.258987",
     "exception": false,
     "start_time": "2022-11-08T11:13:40.219344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain the difference between train, validation and test set\n",
    "* Training set is used for model training \n",
    "* Validation set is used for model fine tuning\n",
    "* Test set is used for model testing. i.e. evaluating the models predictive power and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039579,
     "end_time": "2022-11-08T11:13:40.338452",
     "exception": false,
     "start_time": "2022-11-08T11:13:40.298873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is Naive Bayes algorithm?\n",
    "* Naive Bayes classifier assumes that all the features(predictors) are independent of each other.\n",
    "* Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "* For more details refer: https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039677,
     "end_time": "2022-11-08T11:13:40.418150",
     "exception": false,
     "start_time": "2022-11-08T11:13:40.378473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is the difference between MLOps and DevOps?\n",
    "* MLOps & DevOps have a lot of things in common. However, DevOps include developing and deploying the software application code in production and this code is usually static and does not change rapidly.\n",
    "* MLOps on the other side also includes developing and deploying the ML code in production. However, here the data changes rapidly and the up-gradation of models has to happen more frequently than typical software application code. \n",
    "* Reference: https://360digitmg.com/mlops-interview-questions-answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039783,
     "end_time": "2022-11-08T11:13:40.497997",
     "exception": false,
     "start_time": "2022-11-08T11:13:40.458214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What are the risks associated with Data Science & how MLOps can overcome the same?\n",
    "* Data Science typically has the following issues:\n",
    "    * Model goes down without an alert and becomes unavailable\n",
    "    * Model gives incorrect predictions for a given observation that cannot be scrutinized further\n",
    "    * Model accuracy decreases further as and how time progresses\n",
    "    * Model maintenance also should be done by data scientists, who are expensive\n",
    "    * Model scaling across the organization is not easy\n",
    "* These risks can be addressed by using MLOps.\n",
    "* Reference: https://360digitmg.com/mlops-interview-questions-answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040793,
     "end_time": "2022-11-08T11:13:40.580541",
     "exception": false,
     "start_time": "2022-11-08T11:13:40.539748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain about model/concept drift.\n",
    "* Model drift, sometimes called concept drift, occurs when the model performance during the inference phase (using real-world data) degrades when compared to its performance during the training phase (using historical, labeled data). \n",
    "* It is also known as train/serve skew as the performance of the model is skewed when compared with the training and serving phases. This could be due to many reasons like\n",
    "\n",
    "    * The underlying distribution of data has changed\n",
    "    * Unforeseen events - like a model trained on pre-covid data is expected to perform much worse on data during the COVID-19 pandemic\n",
    "    * Training happened on a limited number of categories but a recent environmental change happened which added another category\n",
    "    * In NLP problems the real world data has significantly more number of tokens that are different from training data\n",
    "* To detect model drift, it is always necessary to keep continuously monitoring the performance of the model. \n",
    "* If there is a sustained degradation of model performance, the cause needs to be investigated and treatment methods need to be applied accordingly which almost always involves model retraining.\n",
    "* Reference: https://360digitmg.com/mlops-interview-questions-answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040263,
     "end_time": "2022-11-08T11:13:40.661728",
     "exception": false,
     "start_time": "2022-11-08T11:13:40.621465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Use NLP to read T & C\n",
    "* Reference: https://dataistdogma.github.io/NLP.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041936,
     "end_time": "2022-11-08T11:13:40.744751",
     "exception": false,
     "start_time": "2022-11-08T11:13:40.702815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "* https://www.youtube.com/watch?v=k6QWYwOvJs0&t=1149s\n",
    "* https://towardsdatascience.com/taking-the-confusion-out-of-confusion-matrices-c1ce054b3d3e\n",
    "* https://kambria.io/blog/confused-about-the-confusion-matrix-learn-all-about-it/#:~:text=Confusion%20matrices%20are%20used%20to,True%20Negatives%20and%20False%20Negatives.\n",
    "* https://projects.uplevel.work/insights/confusion-matrix-accuracy-sensitivity-specificity-precision-f1-score-how-to-interpret\n",
    "* https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/\n",
    "* https://towardsdatascience.com/imbalanced-classification-in-python-smote-enn-method-db5db06b8d50#:~:text=The%20Concept%3A%20Edited%20Nearest%20Neighbor,the%20observation's%20class%20or%20not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 14.071295,
   "end_time": "2022-11-08T11:13:41.858071",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-08T11:13:27.786776",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
